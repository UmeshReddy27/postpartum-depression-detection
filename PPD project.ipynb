{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 100/100 [02:51<00:00,  1.72s/it, loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 0.11191637723706663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 100/100 [02:47<00:00,  1.67s/it, loss=0.0022] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Average Loss: 0.0021976129850372673\n",
      "BERT model saved as distilbert_ppd_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        98\n",
      "           1       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n",
      "Prediction for 'I feel exhausted and cry every night.': High Risk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('postpartum_depression_text_data_v2.csv')\n",
    "\n",
    "# Selecting text feature and target variable\n",
    "text_column = 'Message'\n",
    "target_column = 'Postpartum_Depression_Score'\n",
    "\n",
    "# Handle missing values\n",
    "df[text_column] = df[text_column].fillna('')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[text_column], df[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Force GPU usage if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom dataset class\n",
    "class PPDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=32):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return {\n",
    "            \"input_ids\": encoding['input_ids'].squeeze().to(device),\n",
    "            \"attention_mask\": encoding['attention_mask'].squeeze().to(device),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long).to(device)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = PPDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = PPDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch]).to(device)\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(device)\n",
    "    labels = torch.stack([item['labels'] for item in batch]).to(device)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer & Loss Function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop with Progress Bar\n",
    "for epoch in range(2):  # Change to more epochs if needed\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_loader))\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"distilbert_ppd_model.pth\")\n",
    "print(\"BERT model saved as distilbert_ppd_model.pth\")\n",
    "\n",
    "# Evaluation with Progress Bar\n",
    "model.eval()\n",
    "y_preds, y_trues = [], []\n",
    "progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "with torch.no_grad():\n",
    "    for batch in progress_bar:\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "        y_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        y_trues.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_trues, y_preds))\n",
    "print(\"Classification Report:\\n\", classification_report(y_trues, y_preds))\n",
    "\n",
    "# Prediction Function\n",
    "def predict_text(text):\n",
    "    encoding = tokenizer(text, padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "    return \"High Risk\" if torch.argmax(probs).item() == 1 else \"Low Risk\"\n",
    "\n",
    "# Example Prediction\n",
    "sample_text = \"I am scared\"\n",
    "print(f\"Prediction for '{sample_text}': {predict_text(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using structured features: ['Age', 'BMI_Before_Pregnancy', 'Pregnancy_Complications', 'Employment_Status', 'Household_Income', 'Family_History_of_Depression', 'Sleep_Disruptions_per_Night', 'Support_from_Partner', 'Mode_of_Delivery', 'Diet_Quality', 'Postpartum_Pain_Severity']\n",
      "Structured model saved as structured_xgboost.pkl\n",
      "Accuracy: 0.8915\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      1410\n",
      "           1       0.99      0.64      0.78       590\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.93      0.82      0.85      2000\n",
      "weighted avg       0.91      0.89      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib  # For saving the trained model\n",
    "\n",
    "# Load structured dataset (Replace 'your_dataset.csv' with actual file path)\n",
    "df = pd.read_csv('postpartum_depression_data_expanded.csv')\n",
    "\n",
    "# Selecting structured features (including new features)\n",
    "all_possible_features = ['Age', 'BMI_Before_Pregnancy', 'Gestational_Age_at_Delivery',\n",
    "                         'Pregnancy_Complications', 'Employment_Status', 'Household_Income',\n",
    "                         'Family_History_of_Depression', 'Sleep_Disruptions_per_Night',\n",
    "                         'Support_from_Partner', 'Mode_of_Delivery', 'Diet_Quality', 'Postpartum_Pain_Severity']\n",
    "\n",
    "# Only select columns that exist in the dataset\n",
    "structured_features = [col for col in all_possible_features if col in df.columns]\n",
    "print(\"Using structured features:\", structured_features)\n",
    "\n",
    "# Target variable\n",
    "target = 'Postpartum_Depression_Score'\n",
    "if target not in df.columns:\n",
    "    raise KeyError(f\"Target variable '{target}' not found in dataset\")\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in df[structured_features].select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # Store encoders for future decoding\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[structured_features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Save trained structured model\n",
    "joblib.dump(xgb_model, \"structured_xgboost.pkl\")\n",
    "print(\"Structured model saved as structured_xgboost.pkl\")\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 'Life is getting better': Low Risk\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Life is getting better\"\n",
    "print(f\"Prediction for '{sample_text}': {predict_text(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured model expects 11 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction: High Risk | Score: 0.9849624395370484\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load trained structured model (XGBoost)\n",
    "structured_model = joblib.load(\"structured_xgboost.pkl\")\n",
    "\n",
    "# Check expected number of features\n",
    "expected_features = structured_model.n_features_in_\n",
    "print(f\"Structured model expects {expected_features} features.\")\n",
    "\n",
    "# Load trained NLP model (BERT)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.load_state_dict(torch.load(\"distilbert_ppd_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Prediction function for NLP model\n",
    "def get_nlp_prediction(text):\n",
    "    encoding = tokenizer(text, padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "    return probs[:, 1].item()  # Probability of postpartum depression\n",
    "\n",
    "# Prediction function for structured model\n",
    "def get_structured_prediction(features):\n",
    "    if len(features) != expected_features:\n",
    "        raise ValueError(f\"Feature shape mismatch. Expected {expected_features}, got {len(features)}\")\n",
    "    prob = structured_model.predict_proba([features])[:, 1]  # Probability of postpartum depression\n",
    "    return prob[0]\n",
    "\n",
    "# Function to combine both models' predictions\n",
    "def combined_prediction(structured_features, text, w_nlp=0.6, w_struct=0.4):\n",
    "    prob_nlp = get_nlp_prediction(text)\n",
    "    prob_structured = get_structured_prediction(structured_features)\n",
    "    final_score = (w_nlp * prob_nlp) + (w_struct * prob_structured)\n",
    "    return 1 if final_score > 0.5 else 0, final_score\n",
    "\n",
    "# Example Usage\n",
    "sample_text = \"I feel exhausted and cry every night.\"\n",
    "\n",
    "# Structured features (Ensure it matches expected feature count)\n",
    "# [Age, BMI Before Pregnancy, Gestational Age at Delivery, Pregnancy Complications, \n",
    "#  Employment Status, Household Income, Family History of Depression, Sleep Disruptions per Night, \n",
    "#  Support from Partner, Mode of Delivery, Diet Quality, Postpartum Pain Severity]\n",
    "sample_structured_features = [25, 24.5, 0, 1, 2, 0, 3, 1, 1, 4, 2]  # Adjusted order to match dataset\n",
    "\n",
    "prediction, score = combined_prediction(sample_structured_features, sample_text)\n",
    "print(\"Final Prediction:\", \"High Risk\" if prediction == 1 else \"Low Risk\", \"| Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['structured_xgboost.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.feature_names = structured_features  # Store feature names before saving\n",
    "joblib.dump(xgb_model, \"structured_xgboost.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured model expects 11 features.\n",
      "Structured model feature names: ['Age', 'BMI_Before_Pregnancy', 'Pregnancy_Complications', 'Employment_Status', 'Household_Income', 'Family_History_of_Depression', 'Sleep_Disruptions_per_Night', 'Support_from_Partner', 'Mode_of_Delivery', 'Diet_Quality', 'Postpartum_Pain_Severity']\n"
     ]
    }
   ],
   "source": [
    "# Load trained structured model (XGBoost)\n",
    "structured_model = joblib.load(\"structured_xgboost.pkl\")\n",
    "\n",
    "# Check expected number of features\n",
    "expected_features = getattr(structured_model, \"n_features_in_\", len(sample_structured_features))  # Fallback if missing\n",
    "print(f\"Structured model expects {expected_features} features.\")\n",
    "\n",
    "# Print feature names safely (if available)\n",
    "if hasattr(structured_model, \"feature_names\"):\n",
    "    print(\"Structured model feature names:\", structured_model.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
